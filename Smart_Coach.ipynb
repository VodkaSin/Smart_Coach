{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Smart Coach.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g_IbQPkPVSxR",
        "kgaabAaEe9rh",
        "9_y6ZIrrhZ-y",
        "4AJc_GkqjUIv",
        "DMePgRAEl92Z",
        "cUjO-IpHmHVI",
        "-RIDHvy0nrqE"
      ],
      "authorship_tag": "ABX9TyORi0Irf+bfPwyy0ioa3Epl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VodkaSin/Smart_Coach/blob/main/Smart_Coach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_IbQPkPVSxR"
      },
      "source": [
        "# Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqABktRocVos",
        "outputId": "8b66419e-59e1-4477-dfc7-214fe26d3997"
      },
      "source": [
        "!pip install -q tensorflow==2.6.0 tensorflow-gpu==2.6.0\n",
        "!pip install -q gTTS\n",
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 458.3 MB 10 kB/s \n",
            "\u001b[?25h  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reAF-7dlY4L6"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZZtn6HacbN7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow_docs.vis import embed # What does this do?\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from matplotlib.collections import LineCollection # What does this do?\n",
        "\n",
        "import matplotlib.patches as patches # What does this do?\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio # What does this do?\n",
        "import PIL\n",
        "import io\n",
        "import math\n",
        "import time\n",
        "from IPython.display import display, Javascript, Image, HTML, Audio # When are all these used?\n",
        "from gtts import gTTS\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7dqLZ4wY6zi"
      },
      "source": [
        "# Visualization helper functions\n",
        "\n",
        "1. Streaming APIs\n",
        "2. Recording and display videos\n",
        "3. Smart cropping\n",
        "4. Frame rendering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmrETs3ceMR"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyqTz_0Mccci"
      },
      "source": [
        "# JavaScript to create video stream from webcam\n",
        "def start_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 192,192);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span></span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      video.style.cssText = \"-moz-transform: scale(-1, 1); \\\n",
        "-webkit-transform: scale(-1, 1); -o-transform: scale(-1, 1); \\\n",
        "transform: scale(-1, 1); filter: FlipH;\";\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 192; //video.videoWidth;\n",
        "      captureCanvas.height = 192; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH5UVooPwhde"
      },
      "source": [
        "# Capture frame in bytes from streaming\n",
        "def take_frame(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLBZCxBfcn9w"
      },
      "source": [
        "def js_to_image(js_reply):\n",
        "  \"\"\" Used in main function to convert image types\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BRG image (mirror flipped)\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply['img'].split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  img = cv2.flip(img,1)\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJcIDVNIct7C"
      },
      "source": [
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\" Used in the main function to convert numpy overlay onto the frame\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgaabAaEe9rh"
      },
      "source": [
        "## Recording and displaying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2DwJl_0gE1A"
      },
      "source": [
        "def record_video(video_path):\n",
        "  \"\"\" Called in the main function as an option to record the exercise video for later reference\n",
        "  Params:\n",
        "          video_path: a path name for storing the video\n",
        "  \"\"\"\n",
        "  js=Javascript(\"\"\"\n",
        "    async function recordVideo() {\n",
        "      const options = { mimeType: \"video/webm; codecs=vp9\" };\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      const stopCapture = document.createElement(\"button\");\n",
        "      \n",
        "      capture.textContent = \"Start Recording\";\n",
        "      capture.style.background = \"orange\";\n",
        "      capture.style.color = \"white\";\n",
        "\n",
        "      stopCapture.textContent = \"Stop Recording\";\n",
        "      stopCapture.style.background = \"red\";\n",
        "      stopCapture.style.color = \"white\";\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const recordingVid = document.createElement(\"video\");\n",
        "      video.style.display = 'block';\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio:true, video: true});\n",
        "    \n",
        "      let recorder = new MediaRecorder(stream, options);\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      video.muted = true;\n",
        "\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      await new Promise((resolve) => {\n",
        "        capture.onclick = resolve;\n",
        "      });\n",
        "      recorder.start();\n",
        "      capture.replaceWith(stopCapture);\n",
        "\n",
        "      await new Promise((resolve) => stopCapture.onclick = resolve);\n",
        "      recorder.stop();\n",
        "      let recData = await new Promise((resolve) => recorder.ondataavailable = resolve);\n",
        "      let arrBuff = await recData.data.arrayBuffer();\n",
        "      \n",
        "      // stop the stream and remove the video element\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "\n",
        "      let binaryString = \"\";\n",
        "      let bytes = new Uint8Array(arrBuff);\n",
        "      bytes.forEach((byte) => {\n",
        "        binaryString += String.fromCharCode(byte);\n",
        "      })\n",
        "    return btoa(binaryString);\n",
        "    }\n",
        "  \"\"\")\n",
        "  try:\n",
        "    display(js)\n",
        "    data=eval_js('recordVideo({})')\n",
        "    binary=b64decode(data)\n",
        "    with open(video_path,\"wb\") as video_file:\n",
        "      video_file.write(binary)\n",
        "    print(f\"Finished recording video at:{video_path}\")\n",
        "  except Exception as err:\n",
        "    print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6BIG70Xe9RZ"
      },
      "source": [
        "def show_video(video_path, video_width = 600):\n",
        "  \"\"\" Called when the user wants to review their workout recording and pose estimation\n",
        "  Params:\n",
        "          video_path: local\n",
        "          video_width: default = 600\n",
        "  Returns:\n",
        "          Display the video in cell\n",
        "  \"\"\"\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        " \n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_y6ZIrrhZ-y"
      },
      "source": [
        "## Cropping algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnyKD2UOhqZs"
      },
      "source": [
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region, provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWcfXkPeiD8m"
      },
      "source": [
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "\n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "\n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "\n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEGB0gMSiATH"
      },
      "source": [
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "\n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
        "           MIN_CROP_KEYPOINT_SCORE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xsauaf9iJL3"
      },
      "source": [
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "\n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]\n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] +\n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] +\n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "\n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNMcvxfniLAm"
      },
      "source": [
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  return output_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evfuBCWZiOGX"
      },
      "source": [
        "def run_inference(movenet, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "\n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  # Run model inference.\n",
        "  keypoints_with_scores = movenet(input_image)\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height +\n",
        "        crop_region['height'] * image_height *\n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width +\n",
        "        crop_region['width'] * image_width *\n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "  return keypoints_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJc_GkqjUIv"
      },
      "source": [
        "## Rendering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VLOcHtuAHrd"
      },
      "source": [
        "def draw_keypoints(frame, keypoints, array, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    points = []\n",
        "    for kp in shaped:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            array = cv2.circle(array, (int(kx), int(ky)), 3, (0,255,0), -1)\n",
        "\n",
        "def draw_connections(frame, keypoints, edges, array, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
        "    \n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = shaped[p1]\n",
        "        y2, x2, c2 = shaped[p2]\n",
        "        \n",
        "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
        "            array = cv2.line(array, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww-ZBCQOxUBr"
      },
      "source": [
        "def draw_text(array, org, text, color):\n",
        "  array = cv2.putText(array, text, org, cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1, cv2.LINE_AA )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MPLOMk9ZjCe"
      },
      "source": [
        "# Model algorithm\n",
        "\n",
        "1. Libraries of definitons (joints and angles), \n",
        "2. Model loading\n",
        "3. Angle calculation algorithm\n",
        "4. Squat counter logic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMePgRAEl92Z"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0CrmuIQi_BY"
      },
      "source": [
        "# Dictionary that maps from joint names to keypoint indices.\n",
        "\n",
        "EDGES = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "KEYPOINT_DICT = {\n",
        "    'nose': 0,\n",
        "    'left_eye': 1,\n",
        "    'right_eye': 2,\n",
        "    'left_ear': 3,\n",
        "    'right_ear': 4,\n",
        "    'left_shoulder': 5,\n",
        "    'right_shoulder': 6,\n",
        "    'left_elbow': 7,\n",
        "    'right_elbow': 8,\n",
        "    'left_wrist': 9,\n",
        "    'right_wrist': 10,\n",
        "    'left_hip': 11,\n",
        "    'right_hip': 12,\n",
        "    'left_knee': 13,\n",
        "    'right_knee': 14,\n",
        "    'left_ankle': 15,\n",
        "    'right_ankle': 16\n",
        "}\n",
        "\n",
        "\n",
        "# Note that the image is flipped, here the keys refer to the user's pov\n",
        "THREE_JOINT_INDEX = {\n",
        "    'right_shoulder (H)': (11,5,7),\n",
        "    'right_shoulder (S)': (7,5,6),\n",
        "    'left_shoulder (H)': (12,6,8),\n",
        "    'left_shoulder (S)': (8,6,5),\n",
        "    'right_elbow':(5,7,9),\n",
        "    'left_elbow':(6,8,10),\n",
        "    'right_torso':(5,11,13),\n",
        "    'left_torso':(6,12,14),\n",
        "    'right_knee':(11,13,15),\n",
        "    'left_knee':(12,14,16),\n",
        "    'right_torso turn':(6,5,11),\n",
        "    'left_torso_turn':(5,6,12)\n",
        "}\n",
        "FOUR_JOINT_INDEX ={\n",
        "    'shoulders_hips':(5,6,11,12),\n",
        "    'btw_legs':(11,13,12,14)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUjO-IpHmHVI"
      },
      "source": [
        "## Model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrDji3XgmJ6n"
      },
      "source": [
        "# Grab from TF hub and initialize interpreter\n",
        "!wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/3?lite-format=tflite\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4gJ9XxwmRhA"
      },
      "source": [
        "# Interpreter\n",
        "def movenet(frame):\n",
        "  \"\"\" Runs detection on an input frame\n",
        "  Params:\n",
        "          frame: 192x192 opencv BRG\n",
        "  Returns:\n",
        "          keypoints_with_scores: [index, location, confidence]\n",
        "  \"\"\"  \n",
        "  # Reshape iamge to 192x192x3\n",
        "  img = tf.image.resize_with_pad(np.expand_dims(frame, axis=0), 192,192)\n",
        "  input_image = tf.cast(img, dtype=tf.float32) # Specify data type\n",
        "\n",
        "  # Setup input and output \n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  # Make predictions \n",
        "  interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "  interpreter.invoke()\n",
        "  keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "  return keypoints_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Fq9-q_ntg8"
      },
      "source": [
        "## Squat counter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D39YEsmkZK3m"
      },
      "source": [
        "def find_side_squat(keypoints_with_scores, threshold=0.2):\n",
        "  '''1. Extract the joints needed: 5,6 (shoulders), 11,12 (hips), 13,14 (knees), 15,16 (ankles)\n",
        "     2. Compare the confidence for both sides: use only the side with higher confidence and if any \n",
        "        angle in the confidence side < threshold, output key 'Bad detection'= False\n",
        "     3. Ouput side [keypoints_with_scores(shoulder, hip, knee, ankle)]\n",
        "     Note: here left and right is the opposite of the user's left and right due to the mirroring effect.'''\n",
        "  left = [keypoints_with_scores[0][0][5],keypoints_with_scores[0][0][11],keypoints_with_scores[0][0][13],keypoints_with_scores[0][0][15]]\n",
        "  right = [keypoints_with_scores[0][0][6],keypoints_with_scores[0][0][12],keypoints_with_scores[0][0][14],keypoints_with_scores[0][0][16]]\n",
        "  left_confidence = [left[i][2] for i in range (4)]\n",
        "  right_confidence = [right[i][2] for i in range (4)]\n",
        "  if sum(left_confidence) > sum(right_confidence):\n",
        "    side = np.asarray(left)\n",
        "    confidence = left_confidence\n",
        "  else:\n",
        "    side = np.asarray(right)\n",
        "    confidence = right_confidence\n",
        "  for i in confidence:\n",
        "    if i<threshold:\n",
        "      return None, False\n",
        "  return np.delete(side,2,1), True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOW7RdDigcpN"
      },
      "source": [
        "def squat_angles(side):\n",
        "  torso = getAngle(side[0],side[1],side[2])\n",
        "  knee = getAngle(side[1],side[2],side[3])\n",
        "  return torso, knee"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auhk2YdGFK69"
      },
      "source": [
        "def squat_count(torso, knee):\n",
        "  '''Default: \n",
        "    1. knee flexion below 100 is considered ok, the user should aim for squatting\n",
        "           down deeper, below 60 is excellent mobility.\n",
        "    2. hip flexion is counted when angle smaller than 90 but should be larger than 70\n",
        "    3. getting up, the knee should be 180 in ideal case, 170 is considered OK, same for hip flexion\n",
        "    \n",
        "    Parameters:\n",
        "    angle: [hip flexion left, hip flexion right, knee flexion left, knee flexion right] list of degree\n",
        "    stage = string, indicator for the counter, if 'down' counter+1'''\n",
        "  if knee<110:\n",
        "    stage = 'down'\n",
        "    score = 1\n",
        "    if knee<90 and torso>65:\n",
        "      score = 2\n",
        "  else:\n",
        "    stage = 'up'\n",
        "    score = 0\n",
        "  return stage, score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RIDHvy0nrqE"
      },
      "source": [
        "## Angle calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTbIv83Yn46D"
      },
      "source": [
        "def getAngle(a, b, c):\n",
        "    ang = abs(math.degrees(math.atan2(b[1]-c[1], b[0]-c[0]) - math.atan2(b[1]-a[1], b[0]-a[0])))\n",
        "    return ang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S5loWastc9e"
      },
      "source": [
        "def getAngle2(a,b,c,d):\n",
        "  # For 4 point inputs (two crossing lines)\n",
        "  dx = c[1]-a[1]\n",
        "  dy = c[0]-a[0]\n",
        "  return getAngle(b,a,(d[0]-dx,d[1]-dy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9QsayVAuP8v"
      },
      "source": [
        "def find_angle_3(keypoints_with_scores, threshold=0.1):\n",
        "  angles = {}\n",
        "  for keys, indexs in THREE_JOINT_INDEX.items():\n",
        "    a = []\n",
        "    flag = True\n",
        "    for i in indexs:\n",
        "      if keypoints_with_scores[0][0][i][2]<threshold:\n",
        "        flag = False\n",
        "        break\n",
        "      a.append((keypoints_with_scores[0][0][i][1],keypoints_with_scores[0][0][i][0]))\n",
        "    if flag:\n",
        "      angles[keys] = getAngle(a[0],a[1],a[2])\n",
        "  return angles   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1udy1FkuRsP"
      },
      "source": [
        "def find_angle_4(keypoints_with_scores, threshold=0.1):\n",
        "  angles = {}\n",
        "  for keys, indexs in FOUR_JOINT_INDEX.items():\n",
        "    a = []\n",
        "    flag = True\n",
        "    for i in indexs:\n",
        "      if keypoints_with_scores[0][0][i][2]<threshold:\n",
        "        flag = False\n",
        "        break\n",
        "      a.append((keypoints_with_scores[0][0][i][1],keypoints_with_scores[0][0][i][0]))\n",
        "    if flag:\n",
        "      angles[keys] = getAngle2(a[0],a[1],a[2],a[3])\n",
        "  return angles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kD1IYcwa7mV"
      },
      "source": [
        "# Smart Coach\n",
        "\n",
        "1. Streaming of mirroed web cam input with pose estimation rendered on frame\n",
        "2. Print output for immediate reading\n",
        "3. Text output for record."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "eBkImQOywO3l",
        "outputId": "2b8f8305-15ef-4b44-b964-3d13654f03fa"
      },
      "source": [
        "start_stream()\n",
        "label_html = 'Smart Coach'  #label\n",
        "img_data = ''\n",
        "max_score = 0\n",
        "total_score = 0\n",
        "total_good = 0\n",
        "total_excellent = 0\n",
        "store = np.empty(1)\n",
        "times = np.empty(1)\n",
        "\n",
        "'''image_height, image_width = 192, 192\n",
        "crop_region = init_crop_region(image_height, image_width)'''\n",
        "while True:\n",
        "    js_reply = take_frame(label_html, img_data)\n",
        "    if not js_reply:\n",
        "      print('Streaming has been interupted')\n",
        "      break\n",
        "    start = time.time()\n",
        "    # Process Javascript Object (frame) to OpenCV BRG\n",
        "    frame = js_to_image(js_reply)\n",
        "    # Initialize render array\n",
        "    img_array= np.zeros([192,192,4], dtype=np.uint8)\n",
        "    # Run the interpreter on the BRG image\n",
        "    keypoints_with_scores = movenet(frame)\n",
        "    side, flag = find_side_squat(keypoints_with_scores)\n",
        "    if flag == False:\n",
        "      draw_text(img_array,(10,100),'Please adjust your position', (255,0,0))\n",
        "    else:\n",
        "      torso, knee = squat_angles(side)\n",
        "      stage, score = squat_count(torso, knee)\n",
        "      np.append(store, (torso,knee,stage,score))\n",
        "      # Only count the highest score in one rep and avoid double counting\n",
        "      if stage =='down':\n",
        "        if score == 2 and score > max_score: \n",
        "          max_score = score\n",
        "          total_score += max_score\n",
        "          total_excellent += 1\n",
        "          draw_text(img_array,(50,50),'+2', (255,0,127))\n",
        "        if score == 1 and max_score < 2:\n",
        "          max_score = score\n",
        "      if stage == 'up':\n",
        "        if max_score == 1:\n",
        "          total_score += max_score\n",
        "          total_good += 1\n",
        "          draw_text(img_array,(50,50),'+1', (102,178,255))\n",
        "        score = 0\n",
        "        max_score = 0\n",
        "    # Rendering the keypoints and edges: add to img_array and convert to JS\n",
        "    #draw_keypoints(frame, keypoints_with_scores, img_array, 0.1)\n",
        "    #draw_connections(frame, keypoints_with_scores, EDGES, img_array, 0.1)\n",
        "    draw_text(img_array,(10,20),'Total score: '+str(total_score), (102,255,102))\n",
        "    draw_text(img_array,(10,35),'Total count: '+str(total_good+total_excellent), (255,255,0))\n",
        "    img_array[:,:,3] = (img_array.max(axis = 2)>0).astype(int)*255\n",
        "    img_data = bbox_to_bytes(img_array)\n",
        "    stop = time.time()\n",
        "    duration = stop-start\n",
        "    times = np.append(times, duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 192,192);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span></span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      video.style.cssText = \"-moz-transform: scale(-1, 1); -webkit-transform: scale(-1, 1); -o-transform: scale(-1, 1); transform: scale(-1, 1); filter: FlipH;\";\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 192; //video.videoWidth;\n",
              "      captureCanvas.height = 192; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function takePhoto(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming has been interupted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPfI9fWHXmM4",
        "outputId": "80f83e2c-d545-469b-e7c9-0a59221b2181"
      },
      "source": [
        "print(total_score)\n",
        "print(total_excellent)\n",
        "print(total_good)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Nw4lNDbS0U"
      },
      "source": [
        "# Data collection\n",
        "\n",
        "1. Testing of frame rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "cLuNA1Z141TE",
        "outputId": "08c2a334-6697-42b8-b5a6-cc0614a5e8d3"
      },
      "source": [
        "points = np.size(times)\n",
        "print('Collected points:', points)\n",
        "sorted = np.sort(times)\n",
        "print('Average loop time(s):', np.mean(sorted))\n",
        "print('Standard deviation(s):', np.std(sorted))\n",
        "plt.hist(sorted, density=True, bins=30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected points: 140\n",
            "Average loop time(s): 0.04547532285962786\n",
            "Standard deviation(s): 0.00345223172623255\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}